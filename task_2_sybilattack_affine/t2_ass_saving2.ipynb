{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big loop: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:04, 49.61it/s]\n",
      "/home/janek/anaconda3/envs/dnn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "train | loss: 0.9587: 100%|██████████| 1250/1250 [00:08<00:00, 153.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0083\n",
      "Epoch [1/1], Val Loss: 0.7485\n",
      "big loop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 55.29it/s]\n",
      "train | loss: 0.9280: 100%|██████████| 1250/1250 [00:09<00:00, 138.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0191\n",
      "Epoch [1/1], Val Loss: 0.7571\n",
      "big loop: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 51.66it/s]\n",
      "train | loss: 0.9496: 100%|██████████| 1250/1250 [00:07<00:00, 178.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0871\n",
      "Epoch [1/1], Val Loss: 0.8109\n",
      "big loop: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 54.39it/s]\n",
      "train | loss: 0.8933: 100%|██████████| 1250/1250 [00:08<00:00, 139.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0903\n",
      "Epoch [1/1], Val Loss: 0.8232\n",
      "big loop: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 52.08it/s]\n",
      "train | loss: 1.0903: 100%|██████████| 1250/1250 [00:06<00:00, 179.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0838\n",
      "Epoch [1/1], Val Loss: 0.8077\n",
      "big loop: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 50.52it/s]\n",
      "train | loss: 0.8738: 100%|██████████| 1250/1250 [00:08<00:00, 156.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0946\n",
      "Epoch [1/1], Val Loss: 0.8140\n",
      "big loop: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 55.12it/s]\n",
      "train | loss: 1.0584: 100%|██████████| 1250/1250 [00:08<00:00, 145.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0863\n",
      "Epoch [1/1], Val Loss: 0.7987\n",
      "big loop: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:02, 70.05it/s]\n",
      "train | loss: 0.9271: 100%|██████████| 1250/1250 [00:08<00:00, 150.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.1018\n",
      "Epoch [1/1], Val Loss: 0.8343\n",
      "big loop: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 50.46it/s]\n",
      "train | loss: 0.7240: 100%|██████████| 1250/1250 [00:07<00:00, 168.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 0.9175\n",
      "Epoch [1/1], Val Loss: 0.6545\n",
      "big loop: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 53.03it/s]\n",
      "train | loss: 0.6899: 100%|██████████| 1250/1250 [00:07<00:00, 170.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.0177\n",
      "Epoch [1/1], Val Loss: 0.7533\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "101031",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m id_rep_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m: rep \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, rep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, preds)}\n\u001b[1;32m    108\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/SybilAttack.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m representations \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mid_rep_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    111\u001b[0m np\u001b[38;5;241m.\u001b[39msavez(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./task2_submission.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, ids\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mids, representations\u001b[38;5;241m=\u001b[39mrepresentations)\n",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m id_rep_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m: rep \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, rep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, preds)}\n\u001b[1;32m    108\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/SybilAttack.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m representations \u001b[38;5;241m=\u001b[39m [\u001b[43mid_rep_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mids]\n\u001b[1;32m    111\u001b[0m np\u001b[38;5;241m.\u001b[39msavez(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./task2_submission.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m, ids\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mids, representations\u001b[38;5;241m=\u001b[39mrepresentations)\n",
      "\u001b[0;31mKeyError\u001b[0m: 101031"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from taskdataset import TaskDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from t2_functions import train, validate, l1_reg, lin_augment_affine\n",
    "\n",
    "class RepresentationsDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x[idx], self.y[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "lr = 0.001\n",
    "\n",
    "preds = []\n",
    "ids = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"big loop: {i}\")\n",
    "    folder = os.path.join('./data', 'submit', 'affine', str(1), f'partition_{i}')\n",
    "    with open(f'{folder}/A_train', 'rb') as f:\n",
    "        A_train_reps = pkl.load(f)\n",
    "    with open(f'{folder}/B_train', 'rb') as f:\n",
    "        B_train_reps = pkl.load(f)\n",
    "    with open(f'{folder}/B_test', 'rb') as f:\n",
    "        B_test_reps = pkl.load(f)\n",
    "    with open(f'{folder}/ids_train', 'rb') as f:\n",
    "        ids_train = pkl.load(f)\n",
    "    with open(f'{folder}/ids_test', 'rb') as f:\n",
    "        ids_test = pkl.load(f)\n",
    "\n",
    "    A_train_aug = torch.tensor(A_train_reps, dtype=torch.float32)\n",
    "    B_train_aug = torch.tensor(B_train_reps, dtype=torch.float32)\n",
    "    A_train_aug, B_train_aug = lin_augment_affine(A_train_aug, B_train_aug, new_for_each_pair=1)\n",
    "\n",
    "    train_dataset = RepresentationsDataset(x=A_train_aug, y=B_train_aug)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    lin_net = Linear(384, 384)\n",
    "    lin_empty_net = Linear(384, 384)\n",
    "\n",
    "    optim = torch.optim.Adam(lr=lr, params=lin_net.parameters())\n",
    "\n",
    "    lin_last_net, lin_best_net = train(epochs, optim, criterion, l1_reg, train_loader, train_loader, lin_net, lin_empty_net, reg_lambda=0.0001)\n",
    "\n",
    "    B_reps = np.concatenate([B_train_reps, B_test_reps], axis=0)\n",
    "    B_reps = torch.tensor(B_reps, dtype=torch.float32)\n",
    "\n",
    "    A_preds = lin_last_net(B_reps)\n",
    "    A_preds = A_preds.detach().numpy()\n",
    "    A_preds[:200] = np.array(A_train_reps)\n",
    "    \n",
    "    preds.append(A_preds)\n",
    "\n",
    "    # to delete\n",
    "    # ids_train = np.arange(B_train_reps.shape[0])\n",
    "    # ids_test = np.arange(B_test_reps.shape[0])\n",
    "\n",
    "    ids_iter = np.concatenate([ids_train, ids_test], axis=0)\n",
    "    ids.append(ids_iter)\n",
    "\n",
    "    # Convert the concatenated tensor to a NumPy array\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "ids = np.concatenate(ids, axis=0)\n",
    "\n",
    "id_rep_map = {id: rep for id, rep in zip(ids, preds)}\n",
    "\n",
    "dataset = torch.load(\"./data/SybilAttack.pt\")\n",
    "\n",
    "representations = []\n",
    "for id in dataset.ids:\n",
    "    if id in id_rep_map:\n",
    "        representations.append(id_rep_map[id])\n",
    "    else:\n",
    "        representations.append(preds[0])\n",
    "\n",
    "np.savez(\"./task2_submission.npz\", ids=dataset.ids, representations=representations)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = []\n",
    "for i, id in enumerate(dataset.ids):\n",
    "    if id in id_rep_map:\n",
    "        representations.append(id_rep_map[id])\n",
    "    else:\n",
    "        representations.append(preds[i])\n",
    "\n",
    "np.savez(\"./task2_submission.npz\", ids=dataset.ids, representations=representations)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
