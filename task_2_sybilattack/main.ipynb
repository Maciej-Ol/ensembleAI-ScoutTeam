{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from taskdataset import TaskDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "from t2_functions import partition_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "os.chdir('..')\n",
    "print(os.getcwd())\n",
    "dataset_t1 = torch.load(\"task_1_modelstealing/data/ModelStealingPub.pt\")\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"task_2_sybilattack/\"))\n",
    "\n",
    "from endpoints.requests import sybil, sybil_reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "print(os.getcwd())\n",
    "dataset = torch.load(\"data/SybilAttack.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(dataset.ids)\n",
    "print(len(ids))\n",
    "print(ids[:10])\n",
    "binned_ids = partition_ids(ids, main_bin_num=10, train=0.1, test=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'binary'\n",
    "\n",
    "sybil_reset(home_or_defense='home', binary_or_affine=task)\n",
    "sybil_reset(home_or_defense='defense', binary_or_affine=task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_test = binned_ids[0]\n",
    "print(len(ids_train))\n",
    "print(len(ids_test))\n",
    "A_train_reps = sybil(ids=ids_train,\n",
    "                 home_or_defense='home',\n",
    "                 binary_or_affine=task)\n",
    "\n",
    "B_train_reps = sybil(ids=ids_train,\n",
    "                 home_or_defense='defense',\n",
    "                 binary_or_affine=task)\n",
    "print(f\"A train reps: {len(A_train_reps)}\")\n",
    "\n",
    "A_test_reps = sybil(ids=ids_test,\n",
    "                 home_or_defense='home',\n",
    "                 binary_or_affine=task)\n",
    "\n",
    "B_test_reps = sybil(ids=ids_test,\n",
    "                 home_or_defense='defense',\n",
    "                 binary_or_affine=task)\n",
    "print(f\"A test reps: {len(A_test_reps)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentationsDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x[idx], self.y[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(criterion, loader, net):\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        true = []\n",
    "        pred = []\n",
    "        for x, y in loader:\n",
    "            y_hat = net(x)\n",
    "            true.append(y.numpy())\n",
    "            pred.append(y_hat.numpy())\n",
    "\n",
    "        true = torch.tensor(np.concatenate(true, axis=0))\n",
    "        pred = torch.tensor(np.concatenate(pred, axis=0))\n",
    "\n",
    "        loss = criterion(pred, true)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_reg(model, reg_lambda):\n",
    "    l1_regularization = torch.tensor(0., device=model.parameters().__next__().device)\n",
    "    for param in model.parameters():\n",
    "        l1_regularization += torch.norm(param, p=1)\n",
    "    return reg_lambda * l1_regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(epochs, optim, criterion, regularise, trainloader, valloader, net, empty_net, reg_lambda=None):\n",
    "    best_val_loss = np.inf\n",
    "    best_net = empty_net\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        train_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(trainloader)\n",
    "\n",
    "        for iter, (x, y) in enumerate(progress_bar):\n",
    "            net.train()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            y_hat = net(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            if reg_lambda is not None:\n",
    "                loss += regularise(net, reg_lambda)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            train_loss += batch_loss\n",
    "            if iter % 20 == 0:\n",
    "                progress_bar.set_description(f\"train | loss: {batch_loss:.4f}\")\n",
    "\n",
    "            writer.add_scalar('Training Loss', batch_loss, epoch)\n",
    "\n",
    "        train_loss /= len(trainloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        val_loss = validate(criterion, valloader, net)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        if writer is not None:\n",
    "            # log the validation loss and accuracy\n",
    "            writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return net, best_net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_net = Linear(384, 384)\n",
    "lin_empty_net = Linear(384, 384)\n",
    "\n",
    "mlp = MLP(384, 384, 384)\n",
    "empty_mpl = MLP(384, 384, 384)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RepresentationsDataset(x=A_train_reps, y=B_train_reps)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = RepresentationsDataset(x=A_test_reps, y=B_test_reps)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lr=lr, params=lin_net.parameters())\n",
    "\n",
    "lin_last_net, lin_best_net = train(epochs, optim, criterion, l1_reg, train_loader, test_loader, lin_net, lin_empty_net, reg_lambda=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lr=lr, params=mlp.parameters())\n",
    "\n",
    "mlp_last_net, mlp_best_net = train(epochs, optim, criterion, l1_reg, train_loader, test_loader, mlp, empty_mpl, reg_lambda=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_lin = validate(criterion, test_loader, lin_last_net)\n",
    "print(f\"lin test: {test_loss_lin}\")\n",
    "\n",
    "test_loss_mlp = validate(criterion, test_loader, mlp_last_net)\n",
    "print(f\"lin test: {test_loss_mlp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
