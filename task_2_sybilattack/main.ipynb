{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from taskdataset import TaskDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from endpoints.requests import sybil, sybil_reset\n",
    "import sklearn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Krzysztof\\Desktop\\ensembleAI-ScoutTeam\\task_2_sybilattack\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "dataset = torch.load(r\"data\\SybilAttack.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101031, 8526, 43127, 191394, 298792, 121086, 149475, 102605, 163605, 101855, 135720, 202472, 78948, 205516, 130325, 61352, 12573, 148600, 288522, 121039, 142636, 248729, 98173, 110229, 251315, 294900, 69630, 246410, 205724, 241323, 239570, 74090, 206834, 248228, 277825, 177638, 178460, 195306, 57873, 239078, 142185, 240902, 77409, 114516, 158823, 257739, 162041, 19492, 105759, 300664, 233603, 213129, 28202, 154881, 56113, 218900, 77103, 166599, 97556, 73616, 236198, 166840, 77532, 3669, 191063, 5040, 57826, 74718, 304774, 48059, 21290, 268421, 236305, 229322, 280199, 305084, 107778, 230929, 272095, 141814, 297494, 262306, 7149, 222224, 66291, 104205, 252405, 231398, 45372, 292658, 144859, 201391, 1212, 270096, 287366, 208108, 71574, 260196, 210798, 162229, 292402, 170376, 193023, 6021, 135462, 122707, 298246, 171671, 263538, 76182, 141419, 256928, 293835, 13331, 217190, 19693, 122601, 187403, 35388, 189706, 56506, 130137, 51870, 41849, 81127, 88318, 277129, 254901, 100903, 100569, 195183, 16654, 95058, 98496, 77864, 63712, 57763, 291654, 90045, 29921, 195416, 130341, 89860, 118965, 2673, 55365, 154138, 84535, 232879, 211432, 59998, 305357, 148422, 157578, 196868, 60946, 15257, 209823, 243181, 273094, 15708, 225238, 102152, 255842, 135521, 135041, 36838, 46214, 39079, 155259, 79135, 163374, 172598, 212709, 258352, 239340, 201641, 147056, 156723, 295779, 198963, 69451, 28930, 50934, 227294, 69653, 98943, 238160, 83713, 255378, 42957, 42953, 277919, 181273, 241117, 41348, 128294, 179530, 186396, 139280, 247209, 121541, 122900, 62715, 212884, 85346, 232937, 221410, 144653, 173842, 138017, 143600, 35880, 95148, 175089, 100955, 99239, 206663, 63222, 119454, 50452, 208033, 222507, 109832, 187582, 137234, 134026, 142621, 241549, 98789, 107777, 137219, 154651, 250556, 116971, 81850, 64993, 70732, 132051, 287547, 97605, 215506, 272048, 67119, 33142, 22097, 164878, 65717, 16598, 239242, 165795, 40983, 153810, 180351, 213103, 79104, 212655, 301976, 253681, 37611, 44235, 177119, 27391, 47525, 58333, 284291, 188504, 258385, 105281, 146877, 129811, 63955, 253088, 146115, 63690, 56623, 16649, 44198, 62817, 58563, 94173, 159858, 8879, 147840, 39475, 89166, 213478, 14280, 84470, 76996, 199385, 280186, 107029, 148701, 41495, 162835, 204646, 223293, 102110, 179620, 98206, 162357, 244574, 189757, 252590, 267805, 299545, 182249, 298752, 205707, 29984, 193904, 271239, 176089, 204484, 198849, 55166, 16639, 174058, 189217, 264574, 115942, 16321, 93872, 18749, 243339, 201037, 93628, 235703, 146370, 289820, 202050, 71005, 106223, 53332, 218936, 105589, 294470, 299160, 191406, 60109, 42272, 68013, 128562, 129043, 158438, 229306, 288604, 53596, 188934, 272172, 99433, 10908, 178381, 295607, 6558, 153126, 167392, 242211, 216422, 150443, 133273, 41922, 163746, 160793, 157172, 290268, 230997, 199547, 36184, 208659, 242379, 170233, 182107, 182740, 83741, 106356, 58531, 161427, 162026, 105943, 244872, 57910, 3964, 181319, 203969, 288965, 119323, 169745, 213068, 43705, 191252, 221440, 289752, 126225, 195075, 46686, 156332, 236036, 290461, 142382, 302112, 24388, 252487, 297159, 46994, 193607, 269440, 205347, 131519, 211570, 188873, 23611, 92643, 20430, 218458, 46232, 26603, 43130, 286644, 82030, 257802, 203410, 51642, 85933, 291957, 231570, 6252, 296360, 304645, 185557, 99216, 124284, 279900, 24620, 227188, 126026, 31059, 85619, 294530, 187900, 282041, 297783, 221816, 121514, 290415, 254290, 262409, 122187, 3508, 166866, 225978, 204678, 277319, 91476, 226095, 167289, 123988, 292172, 169041, 5794, 107479, 239542, 122390, 282186, 108864, 293728, 298154, 206294, 258536, 160081, 210890, 164936, 136134, 203023, 18458, 88714, 31195, 71076, 135376, 81981, 248938, 11798, 227162, 138046, 128958, 260690, 8773, 250970, 247620, 141085, 296470, 83437, 156471, 133368, 192549, 174924, 298722, 153307, 97259, 255716, 217964, 12727, 183261, 110397, 220725, 242426, 60349, 169998, 135743, 34718, 26151, 267147, 65005, 247607, 28665, 159794, 189732, 219882, 278966, 290239, 103305, 118458, 135696, 124253, 296958, 265438, 44506, 682, 170815, 158955, 35477, 22506, 304177, 153291, 207794, 45376, 880, 141817, 291022, 181090, 204518, 271009, 158236, 34344, 195134, 298198, 294024, 192756, 280849, 140748, 230339, 222197, 270053, 51363, 297831, 243171, 32214, 28084, 72644, 205415, 182614, 34018, 127529, 107422, 54704, 12193, 125987, 230010, 83111, 62785, 92591, 12006, 149843, 260900, 137583, 170235, 79069, 283132, 238317, 269508, 15540, 293996, 93322, 211908, 221888, 208580, 120868, 298538, 305256, 27600, 71043, 150147, 195627, 35563, 252053, 223397, 292314, 110755, 117323, 122002, 49169, 119083, 42752, 98142, 238408, 243694, 273649, 184781, 124395, 196452, 117682, 228615, 163021, 40945, 290274, 293805, 266302, 43129, 177331, 109335, 145126, 92695, 158272, 75605, 106308, 302772, 226876, 267023, 73689, 251475, 242096, 162431, 205765, 153052, 163224, 264665, 27912, 36866, 254570, 244846, 11824, 300699, 68221, 28075, 203201, 128259, 123500, 1450, 29473, 113384, 146356, 93154, 285222, 22152, 149294, 274406, 187592, 57739, 288441, 236910, 220269, 21631, 194143, 274996, 279450, 301651, 290010, 163291, 245785, 101870, 292253, 179004, 185956, 50644, 55003, 114711, 129218, 211055, 244996, 256950, 226383, 277176, 93732, 81972, 69631, 21647, 92381, 167986, 70866, 41447, 302699, 159374, 146907, 153986, 294017, 235598, 129210, 262184, 295024, 216029, 144874, 72864, 54280, 201383, 130240, 189971, 67624, 32066, 91932, 175353, 95898, 226795, 88268, 198909, 200559, 69592, 284620, 177373, 217433, 300591, 296054, 49113, 106388, 127263, 47193, 188546, 295568, 106937, 28498, 27366, 196848, 88030, 237745, 264541, 130370, 212859, 75553, 130410, 30969, 209510, 225864, 221200, 140713, 120054, 252555, 12921, 278848, 38448, 53682, 244082, 196042, 108871, 285647, 141367, 234973, 219202, 268443, 249212, 170481, 33271, 166080, 104226, 275837, 218355, 179008, 192633, 292153, 196503, 296379, 196437, 256352, 14530, 208675, 225104, 155280, 168358, 177557, 272363, 272826, 202624, 65788, 163591, 7873, 10238, 18923, 221001, 14593, 168418, 197776, 250816, 176745, 71733, 88741, 187745, 149228, 55582, 93424, 253092, 270545, 183000, 177201, 176829, 218196, 54011, 9147, 304524, 195406, 121630, 204495, 282811, 110166, 28818, 20249, 67967, 125141, 240673, 88840, 19029, 3439, 153221, 268888, 12548, 134635, 41592, 57697, 176622, 237508, 108814, 81493, 231709, 304487, 197410, 163211, 73471, 253524, 68849, 258035, 53568, 173976, 118511, 270690, 13503, 111819, 149469, 37025, 202163, 154348, 60904, 61939, 1124, 160735, 279975, 75593, 149934, 265959, 137241, 24544, 116199, 296284, 3554, 147827, 142217, 235091, 182291, 87565, 207178, 227163, 15939, 114568, 31206, 246375, 164876, 208760, 204397, 264644, 249640, 35824, 221694, 80937, 155241, 173523, 97172, 31100, 137712, 70698, 109910, 252339, 279051, 267280, 76980, 121500, 227289, 287289, 91438, 102939, 192522, 106358, 271907, 79419, 27565, 294918, 284473, 204888, 49440, 66217, 243940, 290130, 163990, 82487, 221680, 195885, 156837, 97692, 96861, 64838, 116498, 39776, 111627, 118043, 266303, 181104, 172723, 62223, 4233, 52184, 277340, 44595, 285950, 240553, 214666, 291473, 4130, 53200, 202107, 184105, 127762, 191295, 235597, 126500, 233635, 152011, 256590, 156572, 274421, 291471, 74812, 73155, 206245, 238678, 98822, 290386, 49941, 148677, 196090, 263162, 167066, 284076, 218587, 27936, 285693, 295716, 206433, 47465, 270566, 54221, 35180, 32603, 297409, 184732, 266584, 22452, 226252, 153932, 225810, 224307, 267541, 218140, 24880, 72541, 94257, 250825, 287221, 19138, 277910, 155122, 62476, 175004, 63696, 203332, 230998, 227452, 194810, 38762, 224417, 129476, 178923, 157758, 258453, 80780, 223912, 94823, 165814, 285870, 130166, 170565, 27086, 277331, 119703, 275803, 39200, 178098, 36404, 126068, 155083, 144231, 254554, 236783, 91450, 167792, 97734, 232301, 270419, 297302, 247816, 206141, 296526, 90491, 127829, 76315, 250834, 242150, 255801, 275179, 105999, 48569, 142620, 93143, 158476, 184216, 114624, 112841, 172014, 182903, 267776, 7014, 85168, 2123, 28540, 290722, 83362, 129539, 279811, 258804, 67944, 187284, 265892, 282914, 286127, 5515, 255613, 103714, 248958, 27847, 222637, 297394, 60436, 35112, 273468, 207915, 300123, 232542, 218534, 5779, 127739, 163924, 128946, 99869, 144166, 250319, 234984, 101921, 140097, 149846, 161963, 45194, 33622, 145065, 83597, 184026, 255047, 136630, 78781, 14990, 265323, 95520, 15218, 132867, 115825, 207671, 125759, 179214, 102867, 11849, 277081, 41068, 264227, 205271, 198513, 253828, 296040, 142343, 68896, 261732, 75831, 111665, 270789, 249160, 275900, 32729, 14630, 130990, 7025, 228628, 70945, 140145, 217161, 505, 281005, 233284, 38752, 89348, 215963, 128767, 116488, 9588, 1289, 24535, 179949, 222530, 21938, 128000, 222341, 133084, 219482, 238402, 157012, 38947, 13097, 17560, 109786, 12786, 154569, 125994, 142191, 143469, 158926, 80437, 16415, 16141, 64271, 126851, 297775, 21318, 15695, 299541, 24400, 304419, 126181, 70969, 70085, 258692, 218276, 163045, 235251, 190452, 82905, 83356, 250234, 116879, 233648, 168651, 12695, 206439, 253940, 88234, 122818, 300546, 74315, 252914, 275202, 81974, 203182, 180653, 183371, 155301, 50410, 126907, 63220, 38383, 125478, 59286, 84850, 86289, 73079, 277682, 75490, 106577, 87079, 7059, 112821, 71683, 171315, 279953, 76557, 92318, 82495, 50073, 47418, 253630, 35638, 166514, 215408, 52279, 193755, 202114, 55455, 80154, 83896, 85081, 134692, 110146, 138294, 226209, 70455, 9708, 32185, 232263, 10223, 35389, 130044, 245437, 246489, 43343, 198570, 145069, 145636, 193718, 88868, 175753, 103173, 30581, 193083, 160241, 305330, 270621, 222854, 259973, 142067, 299767, 46251, 130833, 182795, 215441, 144228, 39001, 22801, 244307, 113411, 282132, 212014, 281451, 130262, 107435, 282741, 266745, 2591, 113160, 188172, 48976, 85300, 69418, 102404, 187573, 73787, 259990, 57358, 291050, 143895, 66560, 216564, 129012, 36983, 272084, 264109, 285683, 264948, 182717, 210322, 193942, 89444, 32292, 208357, 127150, 274491, 301231, 65164, 288046, 39308, 101034, 22666, 30833, 234909, 188429, 126794, 290381, 102184, 29795, 230372, 268204, 182691, 291288, 107839, 11252, 241869, 3969, 120769, 101079, 112220, 215077, 20309, 231514, 37797, 277587, 97504, 218307, 166519, 77670, 278935, 111820, 50466, 127686, 216642, 271134, 178678, 176434, 237474, 183528, 278734, 119842, 198229, 220892, 121301, 287021, 16734, 288301, 291803, 96342, 23544, 79872, 68563, 12033, 53962, 250245, 151451, 246817, 223876, 258390, 12758, 220795, 265730, 116462, 173819, 10493, 73610, 79892, 101198, 274793, 93463, 296074, 239971, 299272, 48063, 51722, 20748, 126999, 275769, 227522, 100079, 153378, 223457, 249954, 192386, 155699, 92822, 42224, 35778, 94914, 112974, 164412, 34417, 39332, 282721, 155970, 173814, 289099, 166005, 13509, 223985, 130940, 233827, 94080, 251569, 269969, 177778, 6590, 247367, 72412, 113285, 12146, 119578, 80925, 38471, 147730, 128625, 259526, 229747, 206883, 218674, 186963, 257669, 176452, 291873, 174912, 181061, 279153, 27537, 246604, 251477, 77320, 37599, 271227, 210836, 179752, 80396, 34015, 68434, 79559, 74495, 16342, 222032, 169265, 86937, 111928, 68895, 118353, 294822, 29657, 64407, 138511, 133534, 291235, 173716, 130543, 195148, 259714, 196244, 86264, 154353, 172628, 275537, 278898, 210470, 187389, 111055, 26739, 170866, 38903, 81515, 220136, 28464, 295902, 22596, 100932, 41227, 91266, 19473, 300807, 204643, 98875, 287168, 24884, 190549, 206060, 205572, 280775, 274053, 125575, 127984, 161697, 282240, 69730, 3269, 270546, 224344, 121745, 87275, 122237, 227422, 155543, 41709, 257773, 257907, 215160, 105119, 221833, 23218, 5546, 17385, 264688, 296829, 78951, 210555, 85137, 53803, 148278, 41488, 175998, 150181, 9586, 190718, 140122, 280821, 236878, 275422, 27167, 49082, 118506, 187615, 269334, 253848, 103197, 9489, 134253, 225125, 206815, 305634, 217077, 27079, 176596, 65148, 217951, 174480, 69931, 127893, 53649, 648, 21114, 206646, 304269, 78818, 217117, 257022, 83209, 206389, 297905, 189699, 157338, 16911, 50223, 165786, 149791, 98390, 203421, 134007, 233424, 236488, 12682, 254283, 260148, 38544, 208746, 238514, 172029, 147814, 204842, 175983, 263125, 257055, 155657, 208916, 182368, 41855, 52149, 138608, 287158, 264356, 38357, 114827, 179066, 106086, 74486, 53461, 295217, 139122, 248355, 235156, 287746, 177152, 18305, 268120, 53503, 239939, 40391, 180676, 175130, 73153, 103600, 204124, 49852, 25891, 253505, 164047, 161084, 261073, 298977, 81609, 295489, 37774, 116771, 18480, 103970, 210395, 77855, 79899, 236245, 236314, 188758, 38397, 170628, 239851, 274507, 206023, 164735, 81215, 118362, 93603, 126238, 25737, 213723, 256853, 233111, 39083, 244793, 59050, 303420, 222838, 230146, 5753, 285740, 22003, 138636, 22487, 283964, 108090, 4694, 100532, 44688, 94578, 75527, 186985, 265288, 36606, 286346, 273501, 291802, 199607, 286665, 230161, 208938, 175931, 277146, 1447, 258857, 156460, 110324, 133276, 22838, 73542, 185230, 170118, 48023, 216602, 193036, 76573, 123698, 197435, 125428, 48187, 201711, 115649, 104064, 89697, 285360, 228013, 230252, 285506, 78265, 297391, 172087, 276615, 223852, 44894, 114433, 176344, 161659, 286486, 280202, 138695, 2299, 207728, 23906, 112816, 139114, 121650, 293059, 74708, 136634, 106654, 250549, 142816, 31846, 241936, 270067, 129652, 38247, 299573, 184355, 176448, 177536, 25671, 285104, 286214, 79591, 96275, 95287, 119279, 75763, 204281, 303482, 42726, 171932, 207302, 27177, 115255, 61594, 9254, 61903, 241038, 52658, 92077, 54313, 305342, 6191, 251075, 64528, 272512, 169299, 213273, 181213, 81229, 183873, 108304, 21724, 185963, 28556, 144400, 22939, 38816, 249217, 84978, 57951, 195185, 200353, 262087, 163926, 286311, 263632, 100835, 286313, 185363, 292418, 211812, 150723, 25047, 43062, 271547, 116616, 177230, 74811, 60832, 243414, 3490, 17218, 272407, 291485, 298629, 260277, 278418, 210685, 297565, 155544, 218218, 174798, 25128, 232015, 39782, 7363, 3899, 234344, 227020, 39376, 292976, 139063, 158444, 300550, 174801, 10103, 255605, 222575, 50920, 96528, 244827, 197076, 33284, 117568, 219276, 160911, 75079, 7652, 130668, 247384, 181589, 90975, 75862, 98421, 124099, 214416, 276578, 115119, 149770, 90761, 299107, 90497, 172093, 169029, 277894, 28727, 156492, 144633, 58823, 229049, 180641, 35935, 218831, 275774, 266624, 277381, 137456, 126894, 12951, 22043, 120654, 285743, 193634, 205191, 166159, 227627, 214456, 1262, 81592, 40856, 8795, 236947, 90277, 186088, 24149, 21809, 235677, 234240, 55530, 164503, 130350, 164419, 239682, 91485, 254597, 79810, 169590, 221796, 265024, 33451, 44792, 50248, 246844, 183649, 248933, 270702, 238995, 299101, 214070, 91511, 238268, 131481, 14989, 185980, 239408, 8945, 118836, 214217, 283123, 11989, 15514, 24216, 172176, 85165, 137006, 116903, 279476, 82811, 189743, 234268, 77348, 204710, 246940, 8963, 215982, 49449, 13768, 51085, 79782, 4650, 109243, 205871, 245837, 85997, 202096, 130082, 168373, 48539, 155632, 282380, 86694, 55784, 72837, 189246, 150314, 118927, 271524, 6914, 162773, 93561, 50418, 295235, 127936, 100762, 130919, 43878, 208677, 288363, 485, 282187, 296228, 46481, 130841, 87184, 167486, 216853, 278921, 232538, 2031, 288176, 4747, 199350, 111749, 32302, 36261, 182633, 14937, 169160, 199870, 171636, 103701, 87191, 31225, 267949, 253132, 50302, 293985, 248291]\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset home'}\n"
     ]
    }
   ],
   "source": [
    "index = dataset.ids[:2000]\n",
    "print(index)\n",
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='home')\n",
    "time.sleep(5)\n",
    "api_bin_home = sybil(ids=index,\n",
    "                 home_or_defense='home',\n",
    "                 binary_or_affine='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n"
     ]
    }
   ],
   "source": [
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='defense')\n",
    "time.sleep(10)\n",
    "api_bina = sybil(ids=index,\n",
    "                 home_or_defense='defense',\n",
    "                 binary_or_affine='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sybil_def = np.array(api_bina)\n",
    "bin_sybil_home = np.array(api_bin_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def_shape : (2000, 384), home_shape : (2000, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"def_shape : {bin_sybil_def.shape}, home_shape : {bin_sybil_home.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, shuffle=True, random_seed=None):\n",
    "    \"\"\"\n",
    "    Splits the features (X) and target labels (y) into train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array, the feature matrix.\n",
    "    - y: numpy array, the target vector.\n",
    "    - train_ratio: float, proportion of the dataset for training (default: 0.7).\n",
    "    - val_ratio: float, proportion of the dataset for validation (default: 0.15).\n",
    "    - test_ratio: float, proportion of the dataset for testing (default: 0.15).\n",
    "    - shuffle: bool, whether to shuffle the dataset before splitting (default: True).\n",
    "    - random_seed: int, seed for the random number generator (default: None).\n",
    "\n",
    "    Returns:\n",
    "    - train_X: numpy array, training features.\n",
    "    - val_X: numpy array, validation features.\n",
    "    - test_X: numpy array, test features.\n",
    "    - train_y: numpy array, training target labels.\n",
    "    - val_y: numpy array, validation target labels.\n",
    "    - test_y: numpy array, test target labels.\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "    assert X.shape[0] == y.shape[0], \"Number of samples in X and y must be the same\"\n",
    "    \n",
    "    # Combine X and y for shuffling\n",
    "    data = np.column_stack((X, y))\n",
    "\n",
    "    # Set random seed if provided\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Shuffle the dataset if required\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    # Compute sizes of each split\n",
    "    num_samples = data.shape[0]\n",
    "    train_size = int(train_ratio * num_samples)\n",
    "    val_size = int(val_ratio * num_samples)\n",
    "\n",
    "    x_len, y_len = data.shape\n",
    "    # Split the dataset\n",
    "    train_X, train_y = data[:train_size, :y_len//2], data[:train_size, y_len//2:]\n",
    "    val_X, val_y = data[train_size:train_size + val_size, :y_len//2], data[train_size:train_size + val_size, y_len//2:]\n",
    "    test_X, test_y = data[train_size + val_size:, :y_len//2], data[train_size + val_size:, y_len//2:]\n",
    "\n",
    "    return train_X, val_X, test_X, train_y, val_y, test_y\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target vector\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(bin_sybil_def, bin_sybil_home,train_ratio=0.8,val_ratio = 0., test_ratio = 0.2)\n",
    "#train_data_def, val_data_def, test_data_def = split_data(bin_sybil_def)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def_shape : (1600, 384), home_shape : (1600, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"def_shape : {X_train.shape}, home_shape : {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, num_centers, sigma):\n",
    "        self.num_centers = num_centers\n",
    "        self.sigma = sigma\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "\n",
    "    def _gaussian(self, x, c):\n",
    "        return np.exp(-np.linalg.norm(x - c) ** 2 / (2 * self.sigma ** 2))\n",
    "\n",
    "    def _design_matrix(self, X):\n",
    "        return np.array([self._gaussian(x, c) for c in self.centers] for x in X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.centers = X[np.random.choice(len(X), self.num_centers, replace=False)]\n",
    "        design_matrix = self._design_matrix(X)\n",
    "        self.weights = np.linalg.lstsq(design_matrix, y, rcond=None)[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        design_matrix = self._design_matrix(X)\n",
    "        return design_matrix.dot(self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 384)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "X_train = np.random.rand(100, 2)  # Example training data\n",
    "y_train = np.sin(X_train[:, 0]) + np.cos(X_train[:, 1])  # Example training labels\n",
    "\n",
    "rbf = RBF(num_centers=10, sigma=1.0)\n",
    "rbf.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.random.rand(10, 2)  # Example test data\n",
    "predictions = rbf.predict(X_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        #self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size1, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        #x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg =  MLPRegressor(hidden_layer_sizes=(10,10,10,),random_state=85, max_iter=5000).fit(X_train, y_train)\n",
    "model = MLP(384,512,384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_sm, y_test_sm = X_test[:200], y_test[:200]\n",
    "X_train_sm, y_train_sm = X_train[:200], y_train[:200] \n",
    "#print(f\"shapes {X_test_sm.shape,y_test_sm.shape}\")\n",
    "model_reg_small =  MLPRegressor(hidden_layer_sizes=(512,512,),random_state=85, max_iter=5000).fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_pred = model_reg_small.predict(bin_sybil_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big model: 0.04956420042377787, small model: -0.0570681999287509\n"
     ]
    }
   ],
   "source": [
    "print(f\"Big model: {model_reg.score(X_test, y_test)}, small model: {model_reg_small.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2467848077503865\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array([np.linalg.norm(y_pred-y) for y_pred, y in zip(y_pred,bin_sybil_home)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L1 regularization penalty\n",
    "def l1_penalty(model, l1_lambda):\n",
    "    l1_reg = torch.tensor(0., requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    return l1_lambda * l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 3., 4., 5.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wyn =[]\n",
    "index = np.array([1,2,3])\n",
    "index2 = np.array([3,4,5])\n",
    "out = [[1,2,3],[2,3,4],[4,5,6]]\n",
    "out2 = [[1,2,3],[2,3,4],[4,5,6]]\n",
    "#data = np.hstack((index, out))\n",
    "a=np.concatenate((wyn,index,index2))\n",
    "#outr = np.concatenate((wyn,out,out2))\n",
    "#outr\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset home'}\n",
      "epoch: 0\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 82\u001b[0m\n\u001b[0;32m     67\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mif (epoch+1) % 10 == 0:  # Print every 10 epoch\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    model.eval()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    print(f\"Epoch [{epoch+1}/{num_epochs}], loss whole: {MSE}, loss test: {MSE_t}, train_loss : {loss_tr}]\")\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m out \u001b[38;5;241m=\u001b[39m model(api_bina)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inds:\n\u001b[0;32m     84\u001b[0m     inds \u001b[38;5;241m=\u001b[39m index\n",
      "File \u001b[1;32mc:\\Users\\Krzysztof\\anaconda3\\envs\\praca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Krzysztof\\anaconda3\\envs\\praca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#x = torch.relu(self.fc2(x))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\Krzysztof\\anaconda3\\envs\\praca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Krzysztof\\anaconda3\\envs\\praca\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Krzysztof\\anaconda3\\envs\\praca\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='defense')\n",
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='home')\n",
    "inds = []\n",
    "represen = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"epoch: {i}\")\n",
    "    index = dataset.ids[i*2000:(i+1)*2000]\n",
    "    sybil_reset(binary_or_affine = 'binary', home_or_defense ='defense')\n",
    "    time.sleep(5)\n",
    "    index_home = index[:200]\n",
    "    api_bin_home = np.array(sybil(ids=index_home,\n",
    "                    home_or_defense='home',\n",
    "                    binary_or_affine='binary'))\n",
    "    time.sleep(10)\n",
    "    api_bina = np.array(sybil(ids=index,\n",
    "                    home_or_defense='defense',\n",
    "                    binary_or_affine='binary'))\n",
    "    \n",
    "\n",
    "    model = MLP(384,600,0,384)\n",
    "    X_train_sm, y_train_sm = api_bina[:200], api_bin_home[:200]\n",
    "    #X_test_sm, y_test_sm = bin_sybil_def[200:], bin_sybil_home[200:]\n",
    "\n",
    "    '''\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    '''\n",
    "    #X_test_sm_t, y_test_sm_t = torch.tensor(X_test_sm, dtype=torch.float32),torch.tensor(y_test_sm, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_sm, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_sm, dtype=torch.float32)\n",
    "\n",
    "    #bin_sybil_def_tensor =torch.tensor(bin_sybil_def, dtype=torch.float32)\n",
    "    #bin_sybil_home_tensor =torch.tensor(bin_sybil_home, dtype=torch.float32)\n",
    "\n",
    "    #print(X_train.shape)\n",
    "    #print(X_train_tensor.shape)\n",
    "\n",
    "    # Create DataLoader for batch training\n",
    "    #train_dataset = TensorDataset(X_train_tensor.reshape((384,1600)), y_train_tensor.reshape((384,1600)))\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size =32, shuffle=True)\n",
    "    \n",
    "    api_bina_t = torch.tensor(api_bina)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_MSE = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 60\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            model.train()\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss_tr = loss_MSE(outputs, labels) #+ l1_penalty(model, 1e-7)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss_tr.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        if (epoch+1) % 10 == 0:  # Print every 10 epoch\n",
    "            model.eval()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                out = model(bin_sybil_def_tensor)\n",
    "                MSE = loss_MSE(out,bin_sybil_home_tensor)\n",
    "\n",
    "                out_t = model(X_test_sm_t)\n",
    "                MSE_t = loss_MSE(out_t,y_test_sm_t)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], loss whole: {MSE}, loss test: {MSE_t}, train_loss : {loss_tr}]\")\n",
    "        \"\"\"\n",
    "        out = model(api_bina_t)\n",
    "        if not inds:\n",
    "            inds = index\n",
    "        else:\n",
    "            inds = np.concatenate((inds,index))\n",
    "        \n",
    "        if not represen:\n",
    "            represen = out.detach().numpy()\n",
    "        else:\n",
    "            represen =np.concatenate((represen,out.detach().numpy()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 192)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data/example_submission.npz')\n",
    "data['representations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    np.savez(\n",
    "        \"sybilattack/data/example_submission.npz\",\n",
    "        ids=np.random.permutation(20000),\n",
    "        representations=np.random.randn(20000, 192),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch = 40, loss ~ 0.07, architect 384,600,384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_penalty = l1_weight * sum([p.abs().sum() for p in net.hidden.parameters()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
