{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from taskdataset import TaskDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from endpoints.requests import sybil, sybil_reset\n",
    "import sklearn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Krzysztof\\Desktop\\ensembleAI-ScoutTeam\\task_2_sybilattack\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "dataset = torch.load(r\"data\\SybilAttack.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(index)\\nsybil_reset(binary_or_affine = 'binary', home_or_defense ='home')\\ntime.sleep(5)\\napi_bin_home = sybil(ids=index,\\n                 home_or_defense='home',\\n                 binary_or_affine='binary')\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = dataset.ids[:2000]\n",
    "'''\n",
    "print(index)\n",
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='home')\n",
    "time.sleep(5)\n",
    "api_bin_home = sybil(ids=index,\n",
    "                 home_or_defense='home',\n",
    "                 binary_or_affine='binary')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, shuffle=True, random_seed=None):\n",
    "    \"\"\"\n",
    "    Splits the features (X) and target labels (y) into train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array, the feature matrix.\n",
    "    - y: numpy array, the target vector.\n",
    "    - train_ratio: float, proportion of the dataset for training (default: 0.7).\n",
    "    - val_ratio: float, proportion of the dataset for validation (default: 0.15).\n",
    "    - test_ratio: float, proportion of the dataset for testing (default: 0.15).\n",
    "    - shuffle: bool, whether to shuffle the dataset before splitting (default: True).\n",
    "    - random_seed: int, seed for the random number generator (default: None).\n",
    "\n",
    "    Returns:\n",
    "    - train_X: numpy array, training features.\n",
    "    - val_X: numpy array, validation features.\n",
    "    - test_X: numpy array, test features.\n",
    "    - train_y: numpy array, training target labels.\n",
    "    - val_y: numpy array, validation target labels.\n",
    "    - test_y: numpy array, test target labels.\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
    "    assert X.shape[0] == y.shape[0], \"Number of samples in X and y must be the same\"\n",
    "    \n",
    "    # Combine X and y for shuffling\n",
    "    data = np.column_stack((X, y))\n",
    "\n",
    "    # Set random seed if provided\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Shuffle the dataset if required\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    # Compute sizes of each split\n",
    "    num_samples = data.shape[0]\n",
    "    train_size = int(train_ratio * num_samples)\n",
    "    val_size = int(val_ratio * num_samples)\n",
    "\n",
    "    x_len, y_len = data.shape\n",
    "    # Split the dataset\n",
    "    train_X, train_y = data[:train_size, :y_len//2], data[:train_size, y_len//2:]\n",
    "    val_X, val_y = data[train_size:train_size + val_size, :y_len//2], data[train_size:train_size + val_size, y_len//2:]\n",
    "    test_X, test_y = data[train_size + val_size:, :y_len//2], data[train_size + val_size:, y_len//2:]\n",
    "\n",
    "    return train_X, val_X, test_X, train_y, val_y, test_y\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target vector\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(bin_sybil_def, bin_sybil_home,train_ratio=0.8,val_ratio = 0., test_ratio = 0.2)\n",
    "#train_data_def, val_data_def, test_data_def = split_data(bin_sybil_def)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def_shape : (1600, 384), home_shape : (1600, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"def_shape : {X_train.shape}, home_shape : {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, num_centers, sigma):\n",
    "        self.num_centers = num_centers\n",
    "        self.sigma = sigma\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "\n",
    "    def _gaussian(self, x, c):\n",
    "        return np.exp(-np.linalg.norm(x - c) ** 2 / (2 * self.sigma ** 2))\n",
    "\n",
    "    def _design_matrix(self, X):\n",
    "        return np.array([self._gaussian(x, c) for c in self.centers] for x in X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.centers = X[np.random.choice(len(X), self.num_centers, replace=False)]\n",
    "        design_matrix = self._design_matrix(X)\n",
    "        self.weights = np.linalg.lstsq(design_matrix, y, rcond=None)[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        design_matrix = self._design_matrix(X)\n",
    "        return design_matrix.dot(self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        #self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size1, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        #x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_reg =  MLPRegressor(hidden_layer_sizes=(10,10,10,),random_state=85, max_iter=5000).fit(X_train, y_train)\n",
    "#model = MLP(384,512,384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_sm, y_test_sm = X_test[:200], y_test[:200]\n",
    "#X_train_sm, y_train_sm = X_train[:200], y_train[:200] \n",
    "#print(f\"shapes {X_test_sm.shape,y_test_sm.shape}\")\n",
    "#model_reg_small =  MLPRegressor(hidden_layer_sizes=(512,512,),random_state=85, max_iter=5000).fit(X_train_sm, y_train_sm)\n",
    "\n",
    "#y_pred = model_reg_small.predict(bin_sybil_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L1 regularization penalty\n",
    "def l1_penalty(model, l1_lambda):\n",
    "    l1_reg = torch.tensor(0., requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    return l1_lambda * l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset home'}\n",
      "epoch: 0\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n",
      "epoch: 1\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n",
      "epoch: 2\n",
      "Request ok\n",
      "{'msg': 'Successful sybil binary reset defense'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 86\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    if (epoch+1) % 10 == 0:  # Print every 10 epoch\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m        model.eval()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m        print(f\"Epoch [{epoch+1}/{num_epochs}], loss whole: {MSE}, loss test: {MSE_t}, train_loss : {loss_tr}]\")\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m out \u001b[38;5;241m=\u001b[39m model(api_bina_t)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inds:\n\u001b[0;32m     87\u001b[0m     inds \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "time.sleep(5000)\n",
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='defense')\n",
    "sybil_reset(binary_or_affine = 'binary', home_or_defense ='home')\n",
    "inds = []\n",
    "represen = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"epoch: {i}\")\n",
    "    index = dataset.ids[i*2000:(i+1)*2000]\n",
    "    sybil_reset(binary_or_affine = 'binary', home_or_defense ='defense')\n",
    "    time.sleep(20)\n",
    "    index_home = index[:200]\n",
    "    api_bin_home = np.array(sybil(ids=index_home,\n",
    "                    home_or_defense='home',\n",
    "                    binary_or_affine='binary'))\n",
    "    time.sleep(20)\n",
    "    api_bina = np.array(sybil(ids=index,\n",
    "                    home_or_defense='defense',\n",
    "                    binary_or_affine='binary'))\n",
    "    \n",
    "\n",
    "    model = MLP(384,600,0,384)\n",
    "    X_train_sm, y_train_sm = api_bina[:200], api_bin_home[:200]\n",
    "    #X_test_sm, y_test_sm = bin_sybil_def[200:], bin_sybil_home[200:]\n",
    "\n",
    "    '''\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    '''\n",
    "    #X_test_sm_t, y_test_sm_t = torch.tensor(X_test_sm, dtype=torch.float32),torch.tensor(y_test_sm, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_sm, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_sm, dtype=torch.float32)\n",
    "\n",
    "    #bin_sybil_def_tensor =torch.tensor(bin_sybil_def, dtype=torch.float32)\n",
    "    #bin_sybil_home_tensor =torch.tensor(bin_sybil_home, dtype=torch.float32)\n",
    "\n",
    "    #print(X_train.shape)\n",
    "    #print(X_train_tensor.shape)\n",
    "\n",
    "    # Create DataLoader for batch training\n",
    "    #train_dataset = TensorDataset(X_train_tensor.reshape((384,1600)), y_train_tensor.reshape((384,1600)))\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size =32, shuffle=True)\n",
    "    \n",
    "    api_bina_t = torch.tensor(api_bina,dtype=torch.float32)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_MSE = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 60\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(train_loader):\n",
    "            model.train()\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss_tr = loss_MSE(outputs, labels) #+ l1_penalty(model, 1e-7)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss_tr.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        if (epoch+1) % 10 == 0:  # Print every 10 epoch\n",
    "            model.eval()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                out = model(bin_sybil_def_tensor)\n",
    "                MSE = loss_MSE(out,bin_sybil_home_tensor)\n",
    "\n",
    "                out_t = model(X_test_sm_t)\n",
    "                MSE_t = loss_MSE(out_t,y_test_sm_t)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], loss whole: {MSE}, loss test: {MSE_t}, train_loss : {loss_tr}]\")\n",
    "        \"\"\"\n",
    "    out = model(api_bina_t)\n",
    "    if len(inds):\n",
    "        inds = index\n",
    "    else:\n",
    "        inds = np.concatenate((inds,index))\n",
    "    \n",
    "    if len(represen) == 0:\n",
    "        represen = out.detach().numpy()\n",
    "    else:\n",
    "        represen = np.concatenate((represen,out.detach().numpy()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez(\n",
    "        \"task_2_sybilattack/data/my_submission.npz\",\n",
    "        ids=inds,\n",
    "        representations=represen,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from endpoints.requests import sybil_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sybil_submit(\"task_2_sybilattack/data/my_submission.npz\",\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 192)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data/example_submission.npz')\n",
    "data['representations'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch = 40, loss ~ 0.07, architect 384,600,384"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
